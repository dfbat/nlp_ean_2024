{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aSghTe3WXZj"
   },
   "source": [
    "# Preprocesamiento de Texto\n",
    "\n",
    "## Introducción\n",
    "\n",
    "El preprocesamiento de texto es el primer paso para trabajar con datos textuales en proyectos de Procesamiento de Lenguaje Natural (NLP). Antes de aplicar modelos o algoritmos, es necesario transformar los textos en un formato más estructurado y limpio que permita analizarlos con mayor facilidad. Este proceso ayuda a reducir el ruido en los datos y resaltar lo que es realmente útil para las tareas posteriores.\n",
    "\n",
    "En este cuaderno vamos a explorar técnicas básicas y avanzadas de preprocesamiento aplicadas a un texto literario, utilizando un fragmento de \"Cien Años de Soledad\". El objetivo es aprender a preparar los datos para tareas como clasificación, análisis de sentimiento o generación de texto.\n",
    "\n",
    "### ¿Qué veremos?\n",
    "\n",
    "1. **Normalización del texto**: Minúsculas, eliminación de caracteres innecesarios y formatos consistentes.\n",
    "2. **Tokenización**: Separar el texto en palabras o frases.\n",
    "3. **Eliminación de palabras comunes (stopwords)**: Filtrar palabras que no aportan información útil.\n",
    "4. **Stemming y lematización**: Simplificar palabras a su raíz o forma base.\n",
    "5. **Representación numérica**: Convertir el texto en vectores para análisis cuantitativo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pi8QUfBOWXZk",
    "outputId": "7877fd3f-dbfe-4f2f-f3b5-eef69055280c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dfbat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dfbat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     C:\\Users\\dfbat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package snowball_data is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dfbat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re ## Exprexiones regulares\n",
    "import nltk ## Procesamiento de lenguaje natural\n",
    "from nltk.corpus import stopwords ## Palabras vacias\n",
    "from nltk.stem import SnowballStemmer ## Stemming\n",
    "from nltk.tokenize import word_tokenize ## Tokenizacion\n",
    "from nltk.tokenize import RegexpTokenizer ## Tokenizacion\n",
    "from sklearn.feature_extraction.text import CountVectorizer ## Vectorizador\n",
    "import spacy ## Procesamiento de lenguaje natural\n",
    "\n",
    "############## Descarga de recursos de nltk ################\n",
    "nltk.download('punkt') ## Tokenizador\n",
    "nltk.download('stopwords') ## Palabras vacias\n",
    "nltk.download('snowball_data') ## Stemming\n",
    "nltk.download('wordnet') ## Lematizacion\n",
    "\n",
    "############## Es necesario descargar estos recursos para poder ejecutar el script ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pW656X48WXZl"
   },
   "source": [
    "## 2. Cargar el texto\n",
    "\n",
    "Para este ejercicio inicial usaremos un texto sencillo, el cual se cargará en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "O1MCrRdWWXZl"
   },
   "outputs": [],
   "source": [
    "texto = \"\"\"Hace varios años, en el pelotón de fusilamiento, el coronel Aureliano Buendía había de recordar aquella tarde remota en que su padre lo llevó a conocer el hielo. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='Hace'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encontrar las palabras de la forma Xxxxx usando regexp, la primera mayuscula y las siguientes minusculas \n",
    "re.search('[A-ZÁÉÍÓÚÑÜ][a-záéíóúñü]+', texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hace', 'Aureliano', 'Buendía']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Busca todas las palabras con esos parámetros\n",
    "re.findall('[A-ZÁÉÍÓÚÑÜ][a-záéíóúñü]+', texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "YjUZfS9DWXZl",
    "outputId": "dfae3846-88d5-4ce1-ae20-f53e54118d4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comentario</th>\n",
       "      <th>Sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Me encanta usar las chismofilias, ahora estoy ...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Es increíble, escucho cosas que nunca imaginé....</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La calidad del audio es sorprendente, puedo es...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La batería dura mucho tiempo, lo uso todo el d...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No puedo creer que exista algo así, es como te...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>El diseño es muy discreto, nadie sospecha que ...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>El alcance es limitado, no puedo escuchar lo q...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Es muy caro para lo que ofrece, esperaba más f...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A veces se desconecta y pierdo lo que estaban ...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>No cumple con las expectativas, el sonido es m...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pésimo producto, no entiendo por qué tiene tan...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Las chismofilias son adictivas, no puedo dejar...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Un producto que me ha hecho más feliz, ahora s...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>No es tan bueno como lo pintan, se siente bara...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Me gusta mucho, es una forma divertida de pasa...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Comentario Sentimiento\n",
       "0   Me encanta usar las chismofilias, ahora estoy ...    Positivo\n",
       "1   Es increíble, escucho cosas que nunca imaginé....    Positivo\n",
       "2   La calidad del audio es sorprendente, puedo es...    Positivo\n",
       "3   La batería dura mucho tiempo, lo uso todo el d...    Positivo\n",
       "4   No puedo creer que exista algo así, es como te...    Positivo\n",
       "5   El diseño es muy discreto, nadie sospecha que ...    Positivo\n",
       "6   El alcance es limitado, no puedo escuchar lo q...    Negativo\n",
       "7   Es muy caro para lo que ofrece, esperaba más f...    Negativo\n",
       "8   A veces se desconecta y pierdo lo que estaban ...    Negativo\n",
       "9   No cumple con las expectativas, el sonido es m...    Negativo\n",
       "10  Pésimo producto, no entiendo por qué tiene tan...    Negativo\n",
       "11  Las chismofilias son adictivas, no puedo dejar...    Positivo\n",
       "12  Un producto que me ha hecho más feliz, ahora s...    Positivo\n",
       "13  No es tan bueno como lo pintan, se siente bara...    Negativo\n",
       "14  Me gusta mucho, es una forma divertida de pasa...    Positivo"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## En el caso de un corpus se puede hacer a tra ves de un csv\n",
    "import pandas as pd\n",
    "\n",
    "Corpus_DF=pd.read_csv('https://raw.githubusercontent.com/Izainea/nlp_ean/refs/heads/main/Datos/Datos%20Crudos/corpus_chismofilias.csv')\n",
    "Corpus_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b_o2xtyWXZl"
   },
   "source": [
    "## 3. Normalización\n",
    "\n",
    "La **normalización de texto** se refiere al proceso de transformar un texto para que tenga un formato uniforme y consistente. Esto puede incluir convertir a minúsculas, eliminar caracteres especiales, números o espacios adicionales. Su principal objetivo es reducir la complejidad y estandarizar el texto antes de aplicar técnicas de análisis.\n",
    "\n",
    "### ¿Cuándo usar la normalización?\n",
    "\n",
    "La normalización es útil en varios contextos, pero no siempre es necesaria. Aquí te dejo una guía sobre cuándo es apropiado aplicarla:\n",
    "\n",
    "#### **1. Modelos tradicionales de NLP**\n",
    "- **Modelos como Bag of Words (BoW), TF-IDF o Word2Vec**:\n",
    "  - Estos enfoques se benefician de textos más limpios y homogéneos, ya que cualquier variación (como diferencias en el uso de mayúsculas) puede aumentar innecesariamente la dimensionalidad.\n",
    "  - Ejemplo: `Casa` y `casa` serán consideradas distintas si no se normaliza el texto.\n",
    "\n",
    "#### **2. Análisis de frecuencia o clustering**\n",
    "- En tareas como la identificación de palabras más comunes, búsqueda de temas o clustering de textos, la normalización es esencial para evitar que caracteres especiales o inconsistencias afecten los resultados.\n",
    "\n",
    "#### **3. Modelos avanzados como LLMs (e.g., GPT, BERT)**\n",
    "- Los modelos de lenguaje de gran tamaño ya incluyen mecanismos integrados para manejar textos sin normalizar. **No es necesario preprocesar demasiado**:\n",
    "  - Estos modelos tienen vocabularios entrenados que incluyen variaciones comunes como mayúsculas o caracteres especiales.\n",
    "  - Sin embargo, en algunos casos, eliminar información irrelevante (como emojis o HTML) puede ser beneficioso dependiendo del dominio de los datos.\n",
    "\n",
    "### **¿Qué pasos incluye la normalización?**\n",
    "\n",
    "#### 1. **Conversión a minúsculas**\n",
    "   - Utilidad: Evita que `Casa` y `casa` se traten como diferentes.\n",
    "   - Ejemplo: `\"Hola Mundo\"` → `\"hola mundo\"`\n",
    "\n",
    "#### 2. **Eliminación de caracteres especiales**\n",
    "   - Utilidad: Simplifica el análisis al eliminar puntuación, emojis o símbolos irrelevantes.\n",
    "   - Ejemplo: `\"¡Hola! ¿Cómo estás?\"` → `\"Hola Como estas\"`\n",
    "\n",
    "#### 3. **Eliminación de números (opcional)**\n",
    "   - Utilidad: En textos narrativos o sentimentales, los números suelen no ser informativos.\n",
    "   - Ejemplo: `\"En el año 2023\"` → `\"En el año\"`\n",
    "\n",
    "#### 4. **Eliminación de espacios adicionales**\n",
    "   - Utilidad: Limpia la estructura del texto.\n",
    "   - Ejemplo: `\"Hola     Mundo\"` → `\"Hola Mundo\"`\n",
    "\n",
    "### **¿Cuándo evitar la normalización?**\n",
    "\n",
    "- **Textos donde las mayúsculas son significativas**: En análisis de sentimiento, una frase como `\"¡GRACIAS!\"` puede tener un impacto emocional diferente a `\"gracias\"`.\n",
    "- **Datos con emojis o símbolos relevantes**: En algunos dominios, eliminar emojis podría perder información clave.\n",
    "- **Entradas para LLMs**: Estos modelos ya gestionan caracteres especiales y capitalización, por lo que normalizar excesivamente podría eliminar información útil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nj15wjq1WXZm"
   },
   "source": [
    "<img src=\"https://www.mermaidchart.com/raw/9218311f-e863-4e37-824a-b23305925151?theme=light&version=v0.1&format=svg\" alt=\"Diagrama de flujo para la normalización\" style=\"width:100%; max-width:800px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "16BsPUX5WXZm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hace varios años en el pelotón de fusilamiento el coronel aureliano buendía había de recordar aquella tarde remota en que su padre lo llevó a conocer el hielo'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############## Normalización ################\n",
    "\n",
    "## Convertir a minusculas\n",
    "\n",
    "texto = texto.lower()\n",
    "\n",
    "## Eliminar caracteres especiales\n",
    "texto = re.sub(r\"[\\W_]+\", \" \", texto) ## Elimina caracteres especiales y numeros la expresion regular \\W es para caracteres especiales y \\d para numeros, todo lo que no sea una letra se reemplaza por un espacio.\n",
    "texto.strip() ##elimina espacios al inicio y al final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXB8EU7MeSHY",
    "outputId": "dbf176da-0b27-46ca-d758-27ce0efd6421"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Medellín', 'Márquez', 'México', 'Miguel', 'Miguel', 'Mientras', 'Miguel']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt: Crea una historia donde nombres a muchas personas de habla hispana\n",
    "historia = \"\"\"\n",
    "  En el corazón de la vibrante ciudad de Medellín, vivía una joven llamada Camila.\n",
    "  Ella soñaba con ser escritora y pasaba horas leyendo libros de Gabriel García Márquez.\n",
    "  Su mejor amigo, Santiago, era un talentoso músico que componía melodías conmovedoras.\n",
    "  Juntos, compartían tardes de café con su vecina, una abuela llamada Elena, quien contaba historias de su juventud en la isla de Cuba.\n",
    "  Un día, un misterioso hombre llamado Javier llegó al barrio. Era un artista enigmático que pintaba retratos conmovedores.\n",
    "  Su llegada coincidió con la visita de una familia de México, integrada por Ricardo, su esposa Sofia y sus hijos, Isabella y Miguel.\n",
    "  Camila y Santiago se hicieron amigos de ellos y organizaron una fiesta en la que todos pudieron compartir sus culturas y tradiciones.\n",
    "  Se reunieron alrededor de una mesa llena de sabrosos platillos como tamales, arepas y empanadas.\n",
    "  Isabella, con su gran talento para el baile, aprendió pasos de salsa de manos de un alegre joven llamado Alejandro.\n",
    "  Miguel, un entusiasta de la naturaleza, pasaba horas explorando los jardines junto a Natalia, una bióloga que vivía en la ciudad.\n",
    "  Mientras tanto, Elena seguía contando historias sobre sus viajes a Puerto Rico y Venezuela, fascinando a todos con sus relatos.\n",
    "  En esa época de encuentros y nuevas amistades, Javier encontró la inspiración para su próxima obra,\n",
    "  un retrato colectivo que reflejaba la diversidad y la calidez de la comunidad.\n",
    "  Todos, Camila, Santiago, Elena, Javier, Ricardo, Sofia, Isabella, Miguel, Alejandro, y Natalia,\n",
    "  se convirtieron en una familia extendida, unidos por la música, la literatura, y el arte.\n",
    "  \"\"\"\n",
    "\n",
    "re.findall(r'M[a-záéíóú][a-záéíóú]+', historia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IY8XVDeshbNq",
    "outputId": "a1ea70b6-6d2f-4f39-903a-fcf26ef56f47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '573001234567', '573109876543', '573151112222', '573183334444', '525511112222', '525533334444', '525555556666', '525577778888', '573209990000', '573118887777']\n",
      "Correos electrónicos encontrados:\n",
      "camila.lopez@gmail.com\n",
      "santiago.gomez@hotmail.com\n",
      "elena.rodriguez@outlook.com\n",
      "javier.perez@yahoo.com\n",
      "ricardo.martinez@gmail.com\n",
      "sofia.hernandez@hotmail.com\n",
      "isabella.garcia@outlook.com\n",
      "miguel.lopez@yahoo.com\n",
      "alejandro.sanchez@gmail.com\n",
      "natalia.gomez@hotmail.com\n",
      "\n",
      "Números de teléfono encontrados:\n",
      "\n",
      "573001234567\n",
      "573109876543\n",
      "573151112222\n",
      "573183334444\n",
      "525511112222\n",
      "525533334444\n",
      "525555556666\n",
      "525577778888\n",
      "573209990000\n",
      "573118887777\n"
     ]
    }
   ],
   "source": [
    "# prompt: Has una historia donde nombro muchos correos electrónicos y telefonos\n",
    "\n",
    "historia = \"\"\"\n",
    "  En el corazón de la vibrante ciudad de Medellín, vivía una joven llamada Camila.\n",
    "  Ella soñaba con ser escritora y pasaba horas leyendo libros de Gabriel García Márquez.\n",
    "  Su mejor amigo, Santiago, era un talentoso músico que componía melodías conmovedoras.\n",
    "  Juntos, compartían tardes de café con su vecina, una abuela llamada Elena, quien contaba historias de su juventud en la isla de Cuba.\n",
    "  Un día, un misterioso hombre llamado Javier llegó al barrio. Era un artista enigmático que pintaba retratos conmovedores.\n",
    "  Su llegada coincidió con la visita de una familia de México, integrada por Ricardo, su esposa Sofia y sus hijos, Isabella y Miguel.\n",
    "  Camila y Santiago se hicieron amigos de ellos y organizaron una fiesta en la que todos pudieron compartir sus culturas y tradiciones.\n",
    "  Se reunieron alrededor de una mesa llena de sabrosos platillos como tamales, arepas y empanadas.\n",
    "  Isabella, con su gran talento para el baile, aprendió pasos de salsa de manos de un alegre joven llamado Alejandro.\n",
    "  Miguel, un entusiasta de la naturaleza, pasaba horas explorando los jardines junto a Natalia, una bióloga que vivía en la ciudad.\n",
    "  Mientras tanto, Elena seguía contando historias sobre sus viajes a Puerto Rico y Venezuela, fascinando a todos con sus relatos.\n",
    "  En esa época de encuentros y nuevas amistades, Javier encontró la inspiración para su próxima obra,\n",
    "  un retrato colectivo que reflejaba la diversidad y la calidez de la comunidad.\n",
    "  Todos, Camila, Santiago, Elena, Javier, Ricardo, Sofia, Isabella, Miguel, Alejandro, y Natalia,\n",
    "  se convirtieron en una familia extendida, unidos por la música, la literatura, y el arte.\n",
    "  Camila: camila.lopez@gmail.com, +57 300 123 4567\n",
    "  Santiago: santiago.gomez@hotmail.com, +57 310 9876543\n",
    "  Elena: elena.rodriguez@outlook.com, +57 315 111 2222\n",
    "  Javier: javier.perez@yahoo.com, +57 318 333 4444\n",
    "  Ricardo: ricardo.martinez@gmail.com, +52 55 11112222\n",
    "  Sofia: sofia.hernandez@hotmail.com, +52 55 3333 4444\n",
    "  Isabella: isabella.garcia@outlook.com, +52 55 5555 6666\n",
    "  Miguel: miguel.lopez@yahoo.com, +52 55 7777 8888\n",
    "  Alejandro: alejandro.sanchez@gmail.com, +57 320 999 0000\n",
    "  Natalia: natalia.gomez@hotmail.com, +57 311888 7777\n",
    "\"\"\"\n",
    "\n",
    "# Expresiones regulares para encontrar correos electrónicos y teléfonos\n",
    "emails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', historia)\n",
    "telefonos = re.findall(r'\\+*[0-9]+', historia)\n",
    "\n",
    "telefonos=''.join(telefonos)\n",
    "telefonos=telefonos.split('+')\n",
    "print(telefonos)\n",
    "\n",
    "print(\"Correos electrónicos encontrados:\")\n",
    "for email in emails:\n",
    "  print(email)\n",
    "\n",
    "print(\"\\nNúmeros de teléfono encontrados:\")\n",
    "for telefono in telefonos:\n",
    "  print(telefono)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 780
    },
    "id": "lOQ-3gxvWXZm",
    "outputId": "3392f680-a702-4cc0-8a9d-c51b0302e609"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comentario</th>\n",
       "      <th>Sentimiento</th>\n",
       "      <th>Comentario Limpio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Me encanta usar las chismofilias, ahora estoy ...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>me encanta usar las chismofilias ahora estoy a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Es increíble, escucho cosas que nunca imaginé....</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>es increíble escucho cosas que nunca imaginé m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La calidad del audio es sorprendente, puedo es...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>la calidad del audio es sorprendente puedo esc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La batería dura mucho tiempo, lo uso todo el d...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>la batería dura mucho tiempo lo uso todo el dí...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No puedo creer que exista algo así, es como te...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>no puedo creer que exista algo así es como ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>El diseño es muy discreto, nadie sospecha que ...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>el diseño es muy discreto nadie sospecha que e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>El alcance es limitado, no puedo escuchar lo q...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>el alcance es limitado no puedo escuchar lo qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Es muy caro para lo que ofrece, esperaba más f...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>es muy caro para lo que ofrece esperaba más fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A veces se desconecta y pierdo lo que estaban ...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>a veces se desconecta y pierdo lo que estaban ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>No cumple con las expectativas, el sonido es m...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>no cumple con las expectativas el sonido es ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pésimo producto, no entiendo por qué tiene tan...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>pésimo producto no entiendo por qué tiene tant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Las chismofilias son adictivas, no puedo dejar...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>las chismofilias son adictivas no puedo dejar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Un producto que me ha hecho más feliz, ahora s...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>un producto que me ha hecho más feliz ahora sé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>No es tan bueno como lo pintan, se siente bara...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>no es tan bueno como lo pintan se siente barat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Me gusta mucho, es una forma divertida de pasa...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>me gusta mucho es una forma divertida de pasar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Comentario Sentimiento  \\\n",
       "0   Me encanta usar las chismofilias, ahora estoy ...    Positivo   \n",
       "1   Es increíble, escucho cosas que nunca imaginé....    Positivo   \n",
       "2   La calidad del audio es sorprendente, puedo es...    Positivo   \n",
       "3   La batería dura mucho tiempo, lo uso todo el d...    Positivo   \n",
       "4   No puedo creer que exista algo así, es como te...    Positivo   \n",
       "5   El diseño es muy discreto, nadie sospecha que ...    Positivo   \n",
       "6   El alcance es limitado, no puedo escuchar lo q...    Negativo   \n",
       "7   Es muy caro para lo que ofrece, esperaba más f...    Negativo   \n",
       "8   A veces se desconecta y pierdo lo que estaban ...    Negativo   \n",
       "9   No cumple con las expectativas, el sonido es m...    Negativo   \n",
       "10  Pésimo producto, no entiendo por qué tiene tan...    Negativo   \n",
       "11  Las chismofilias son adictivas, no puedo dejar...    Positivo   \n",
       "12  Un producto que me ha hecho más feliz, ahora s...    Positivo   \n",
       "13  No es tan bueno como lo pintan, se siente bara...    Negativo   \n",
       "14  Me gusta mucho, es una forma divertida de pasa...    Positivo   \n",
       "\n",
       "                                    Comentario Limpio  \n",
       "0   me encanta usar las chismofilias ahora estoy a...  \n",
       "1   es increíble escucho cosas que nunca imaginé m...  \n",
       "2   la calidad del audio es sorprendente puedo esc...  \n",
       "3   la batería dura mucho tiempo lo uso todo el dí...  \n",
       "4   no puedo creer que exista algo así es como ten...  \n",
       "5   el diseño es muy discreto nadie sospecha que e...  \n",
       "6   el alcance es limitado no puedo escuchar lo qu...  \n",
       "7   es muy caro para lo que ofrece esperaba más fu...  \n",
       "8   a veces se desconecta y pierdo lo que estaban ...  \n",
       "9   no cumple con las expectativas el sonido es ma...  \n",
       "10  pésimo producto no entiendo por qué tiene tant...  \n",
       "11  las chismofilias son adictivas no puedo dejar ...  \n",
       "12  un producto que me ha hecho más feliz ahora sé...  \n",
       "13  no es tan bueno como lo pintan se siente barat...  \n",
       "14  me gusta mucho es una forma divertida de pasar...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Ahora hagamos ese proceso de normalización para todo el corpus\n",
    "\n",
    "## Convert\n",
    "\n",
    "Corpus_DF['Comentario Limpio'] = Corpus_DF['Comentario'].str.lower()\n",
    "\n",
    "## Eliminar caracteres especiales\n",
    "\n",
    "Corpus_DF['Comentario Limpio'] = Corpus_DF['Comentario Limpio'].apply(lambda x: re.sub(r\"[\\W_]+\", \" \", x))\n",
    "\n",
    "Corpus_DF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xJpGCwWWXZm"
   },
   "source": [
    "### Tokenización\n",
    "\n",
    "La **tokenización** es el proceso de dividir un texto en unidades más pequeñas llamadas **tokens**. Estos pueden ser palabras, frases o incluso caracteres individuales, dependiendo del enfoque. Este paso es fundamental en Procesamiento de Lenguaje Natural (NLP) porque transforma el texto en fragmentos manejables que los algoritmos pueden analizar y procesar.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Por qué es importante la tokenización?\n",
    "\n",
    "1. **Transformación estructurada**:\n",
    "   - Convierte el texto no estructurado en datos que pueden procesarse de manera computacional.\n",
    "   - Ejemplo:\n",
    "     ```python\n",
    "     texto = \"El gato está sobre la mesa.\"\n",
    "     tokens = [\"El\", \"gato\", \"está\", \"sobre\", \"la\", \"mesa\"]\n",
    "     ```\n",
    "\n",
    "2. **Preparación para análisis posterior**:\n",
    "   - Muchos algoritmos de NLP, como Bag of Words (BoW) o TF-IDF, requieren que el texto esté tokenizado para calcular frecuencias o relaciones entre palabras.\n",
    "\n",
    "3. **Reducción de complejidad**:\n",
    "   - Ayuda a simplificar problemas complejos al dividirlos en componentes más pequeños y manejables.\n",
    "\n",
    "4. **Facilita el preprocesamiento**:\n",
    "   - La tokenización es un paso previo a tareas como la eliminación de palabras vacías (stopwords), stemming, lematización o creación de embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Cómo se realiza la tokenización?\n",
    "\n",
    "Existen diversas técnicas de tokenización, desde las más simples hasta las más avanzadas:\n",
    "\n",
    "1. **Basada en espacios**:\n",
    "   - Divide las palabras donde hay espacios.\n",
    "   - Ejemplo: `\"Hola, mundo\"` → `[\"Hola,\", \"mundo\"]`\n",
    "   - Problema: Incluye signos de puntuación como parte de los tokens.\n",
    "\n",
    "2. **Basada en expresiones regulares**:\n",
    "   - Usa patrones para identificar palabras, números o caracteres específicos.\n",
    "   - Ejemplo: Eliminando puntuación:\n",
    "     ```python\n",
    "     import re\n",
    "     texto = \"Hola, mundo.\"\n",
    "     tokens = re.findall(r'\\w+', texto)  # [\"Hola\", \"mundo\"]\n",
    "     ```\n",
    "\n",
    "3. **Tokenización de subpalabras**:\n",
    "   - Divide las palabras en fragmentos más pequeños (subword units), como en BERT o WordPiece.\n",
    "   - Ejemplo:\n",
    "     - Palabra: `\"corriendo\"`\n",
    "     - Tokens: `[\"corr\", \"iendo\"]`\n",
    "   - Ventaja: Maneja vocabularios más pequeños y es más robusto frente a palabras desconocidas.\n",
    "\n",
    "4. **Tokenización basada en caracteres**:\n",
    "   - Divide el texto en caracteres individuales.\n",
    "   - Útil en tareas que analizan patrones a nivel de carácter.\n",
    "\n",
    "---\n",
    "\n",
    "### Tokenización en modelos avanzados como BERT\n",
    "\n",
    "Modelos avanzados como **BERT, GPT o RoBERTa** tienen sus propias estrategias de tokenización diseñadas para maximizar la eficiencia y la cobertura del vocabulario:\n",
    "\n",
    "1. **Vocabulario fijo**:\n",
    "   - Utilizan un vocabulario limitado, generado previamente, que incluye palabras comunes y fragmentos de palabras.\n",
    "   - Ejemplo: `\"imposible\"` → `[\"im\", \"pos\", \"ible\"]`\n",
    "\n",
    "2. **Manejo de palabras desconocidas**:\n",
    "   - Si una palabra no está en el vocabulario, el modelo la divide en subpalabras para asegurar su representación.\n",
    "\n",
    "3. **Codificación específica (WordPiece, Byte-Pair Encoding)**:\n",
    "   - Estas técnicas dividen las palabras en subword units basadas en frecuencias observadas durante el entrenamiento.\n",
    "   - Ejemplo: `\"autonomía\"` podría ser dividida en `[\"auto\", \"nomía\"]`.\n",
    "\n",
    "4. **Preservación de contexto**:\n",
    "   - La tokenización específica permite que estos modelos capturen relaciones entre palabras y subpalabras, mejorando el análisis semántico.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Cuándo usar tokenización personalizada?\n",
    "\n",
    "1. **Modelos preentrenados (e.g., BERT, GPT)**:\n",
    "   - **No necesitas tokenización manual**. Estos modelos ya incluyen su propio tokenizador, optimizado para su arquitectura y vocabulario.\n",
    "   - Ejemplo con el tokenizador de BERT:\n",
    "     ```python\n",
    "     from transformers import BertTokenizer\n",
    "     tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "     tokens = tokenizer.tokenize(\"El gato está sobre la mesa.\")\n",
    "     print(tokens)  # ['el', 'gato', 'está', 'sobre', 'la', 'mesa', '.']\n",
    "     ```\n",
    "\n",
    "2. **Modelos tradicionales (TF-IDF, BoW)**:\n",
    "   - Aquí se recomienda una tokenización básica o basada en expresiones regulares.\n",
    "\n",
    "3. **Textos en lenguajes complejos (e.g., Chino, Japonés)**:\n",
    "   - La tokenización puede requerir herramientas específicas como **Jieba** o **MeCab** para manejar caracteres que no están separados por espacios.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "La tokenización es ¡importantísima! en NLP porque permite estructurar el texto para tareas posteriores. Sin embargo, **la forma de tokenizar depende del modelo y el problema**:\n",
    "\n",
    "- **Modelos tradicionales** requieren una tokenización explícita.\n",
    "- **Modelos avanzados como BERT** incluyen su propio tokenizador optimizado, lo que simplifica el preprocesamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM1h42RsWXZm"
   },
   "source": [
    "<img src=\"https://www.mermaidchart.com/raw/c333babf-e00a-4830-9c01-ec02d8e4b8aa?theme=light&version=v0.1&format=svg\" alt=\"Diagrama de flujo para la normalización\" style=\"width:100%; max-width:1200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tg39ahppWXZn",
    "outputId": "283f3843-c606-4294-d500-fbcfdc935bb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dfbat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['En',\n",
       " 'el',\n",
       " 'corazón',\n",
       " 'de',\n",
       " 'la',\n",
       " 'vibrante',\n",
       " 'ciudad',\n",
       " 'de',\n",
       " 'Medellín',\n",
       " ',',\n",
       " 'vivía',\n",
       " 'una',\n",
       " 'joven',\n",
       " 'llamada',\n",
       " 'Camila',\n",
       " '.',\n",
       " 'Ella',\n",
       " 'soñaba',\n",
       " 'con',\n",
       " 'ser',\n",
       " 'escritora',\n",
       " 'y',\n",
       " 'pasaba',\n",
       " 'horas',\n",
       " 'leyendo',\n",
       " 'libros',\n",
       " 'de',\n",
       " 'Gabriel',\n",
       " 'García',\n",
       " 'Márquez',\n",
       " '.',\n",
       " 'Su',\n",
       " 'mejor',\n",
       " 'amigo',\n",
       " ',',\n",
       " 'Santiago',\n",
       " ',',\n",
       " 'era',\n",
       " 'un',\n",
       " 'talentoso',\n",
       " 'músico',\n",
       " 'que',\n",
       " 'componía',\n",
       " 'melodías',\n",
       " 'conmovedoras',\n",
       " '.',\n",
       " 'Juntos',\n",
       " ',',\n",
       " 'compartían',\n",
       " 'tardes',\n",
       " 'de',\n",
       " 'café',\n",
       " 'con',\n",
       " 'su',\n",
       " 'vecina',\n",
       " ',',\n",
       " 'una',\n",
       " 'abuela',\n",
       " 'llamada',\n",
       " 'Elena',\n",
       " ',',\n",
       " 'quien',\n",
       " 'contaba',\n",
       " 'historias',\n",
       " 'de',\n",
       " 'su',\n",
       " 'juventud',\n",
       " 'en',\n",
       " 'la',\n",
       " 'isla',\n",
       " 'de',\n",
       " 'Cuba',\n",
       " '.',\n",
       " 'Un',\n",
       " 'día',\n",
       " ',',\n",
       " 'un',\n",
       " 'misterioso',\n",
       " 'hombre',\n",
       " 'llamado',\n",
       " 'Javier',\n",
       " 'llegó',\n",
       " 'al',\n",
       " 'barrio',\n",
       " '.',\n",
       " 'Era',\n",
       " 'un',\n",
       " 'artista',\n",
       " 'enigmático',\n",
       " 'que',\n",
       " 'pintaba',\n",
       " 'retratos',\n",
       " 'conmovedores',\n",
       " '.',\n",
       " 'Su',\n",
       " 'llegada',\n",
       " 'coincidió',\n",
       " 'con',\n",
       " 'la',\n",
       " 'visita',\n",
       " 'de',\n",
       " 'una',\n",
       " 'familia',\n",
       " 'de',\n",
       " 'México',\n",
       " ',',\n",
       " 'integrada',\n",
       " 'por',\n",
       " 'Ricardo',\n",
       " ',',\n",
       " 'su',\n",
       " 'esposa',\n",
       " 'Sofia',\n",
       " 'y',\n",
       " 'sus',\n",
       " 'hijos',\n",
       " ',',\n",
       " 'Isabella',\n",
       " 'y',\n",
       " 'Miguel',\n",
       " '.',\n",
       " 'Camila',\n",
       " 'y',\n",
       " 'Santiago',\n",
       " 'se',\n",
       " 'hicieron',\n",
       " 'amigos',\n",
       " 'de',\n",
       " 'ellos',\n",
       " 'y',\n",
       " 'organizaron',\n",
       " 'una',\n",
       " 'fiesta',\n",
       " 'en',\n",
       " 'la',\n",
       " 'que',\n",
       " 'todos',\n",
       " 'pudieron',\n",
       " 'compartir',\n",
       " 'sus',\n",
       " 'culturas',\n",
       " 'y',\n",
       " 'tradiciones',\n",
       " '.',\n",
       " 'Se',\n",
       " 'reunieron',\n",
       " 'alrededor',\n",
       " 'de',\n",
       " 'una',\n",
       " 'mesa',\n",
       " 'llena',\n",
       " 'de',\n",
       " 'sabrosos',\n",
       " 'platillos',\n",
       " 'como',\n",
       " 'tamales',\n",
       " ',',\n",
       " 'arepas',\n",
       " 'y',\n",
       " 'empanadas',\n",
       " '.',\n",
       " 'Isabella',\n",
       " ',',\n",
       " 'con',\n",
       " 'su',\n",
       " 'gran',\n",
       " 'talento',\n",
       " 'para',\n",
       " 'el',\n",
       " 'baile',\n",
       " ',',\n",
       " 'aprendió',\n",
       " 'pasos',\n",
       " 'de',\n",
       " 'salsa',\n",
       " 'de',\n",
       " 'manos',\n",
       " 'de',\n",
       " 'un',\n",
       " 'alegre',\n",
       " 'joven',\n",
       " 'llamado',\n",
       " 'Alejandro',\n",
       " '.',\n",
       " 'Miguel',\n",
       " ',',\n",
       " 'un',\n",
       " 'entusiasta',\n",
       " 'de',\n",
       " 'la',\n",
       " 'naturaleza',\n",
       " ',',\n",
       " 'pasaba',\n",
       " 'horas',\n",
       " 'explorando',\n",
       " 'los',\n",
       " 'jardines',\n",
       " 'junto',\n",
       " 'a',\n",
       " 'Natalia',\n",
       " ',',\n",
       " 'una',\n",
       " 'bióloga',\n",
       " 'que',\n",
       " 'vivía',\n",
       " 'en',\n",
       " 'la',\n",
       " 'ciudad',\n",
       " '.',\n",
       " 'Mientras',\n",
       " 'tanto',\n",
       " ',',\n",
       " 'Elena',\n",
       " 'seguía',\n",
       " 'contando',\n",
       " 'historias',\n",
       " 'sobre',\n",
       " 'sus',\n",
       " 'viajes',\n",
       " 'a',\n",
       " 'Puerto',\n",
       " 'Rico',\n",
       " 'y',\n",
       " 'Venezuela',\n",
       " ',',\n",
       " 'fascinando',\n",
       " 'a',\n",
       " 'todos',\n",
       " 'con',\n",
       " 'sus',\n",
       " 'relatos',\n",
       " '.',\n",
       " 'En',\n",
       " 'esa',\n",
       " 'época',\n",
       " 'de',\n",
       " 'encuentros',\n",
       " 'y',\n",
       " 'nuevas',\n",
       " 'amistades',\n",
       " ',',\n",
       " 'Javier',\n",
       " 'encontró',\n",
       " 'la',\n",
       " 'inspiración',\n",
       " 'para',\n",
       " 'su',\n",
       " 'próxima',\n",
       " 'obra',\n",
       " ',',\n",
       " 'un',\n",
       " 'retrato',\n",
       " 'colectivo',\n",
       " 'que',\n",
       " 'reflejaba',\n",
       " 'la',\n",
       " 'diversidad',\n",
       " 'y',\n",
       " 'la',\n",
       " 'calidez',\n",
       " 'de',\n",
       " 'la',\n",
       " 'comunidad',\n",
       " '.',\n",
       " 'Todos',\n",
       " ',',\n",
       " 'Camila',\n",
       " ',',\n",
       " 'Santiago',\n",
       " ',',\n",
       " 'Elena',\n",
       " ',',\n",
       " 'Javier',\n",
       " ',',\n",
       " 'Ricardo',\n",
       " ',',\n",
       " 'Sofia',\n",
       " ',',\n",
       " 'Isabella',\n",
       " ',',\n",
       " 'Miguel',\n",
       " ',',\n",
       " 'Alejandro',\n",
       " ',',\n",
       " 'y',\n",
       " 'Natalia',\n",
       " ',',\n",
       " 'se',\n",
       " 'convirtieron',\n",
       " 'en',\n",
       " 'una',\n",
       " 'familia',\n",
       " 'extendida',\n",
       " ',',\n",
       " 'unidos',\n",
       " 'por',\n",
       " 'la',\n",
       " 'música',\n",
       " ',',\n",
       " 'la',\n",
       " 'literatura',\n",
       " ',',\n",
       " 'y',\n",
       " 'el',\n",
       " 'arte',\n",
       " '.',\n",
       " 'Camila',\n",
       " ':',\n",
       " 'camila.lopez',\n",
       " '@',\n",
       " 'gmail.com',\n",
       " ',',\n",
       " '+57',\n",
       " '300',\n",
       " '123',\n",
       " '4567',\n",
       " 'Santiago',\n",
       " ':',\n",
       " 'santiago.gomez',\n",
       " '@',\n",
       " 'hotmail.com',\n",
       " ',',\n",
       " '+57',\n",
       " '310',\n",
       " '9876543',\n",
       " 'Elena',\n",
       " ':',\n",
       " 'elena.rodriguez',\n",
       " '@',\n",
       " 'outlook.com',\n",
       " ',',\n",
       " '+57',\n",
       " '315',\n",
       " '111',\n",
       " '2222',\n",
       " 'Javier',\n",
       " ':',\n",
       " 'javier.perez',\n",
       " '@',\n",
       " 'yahoo.com',\n",
       " ',',\n",
       " '+57',\n",
       " '318',\n",
       " '333',\n",
       " '4444',\n",
       " 'Ricardo',\n",
       " ':',\n",
       " 'ricardo.martinez',\n",
       " '@',\n",
       " 'gmail.com',\n",
       " ',',\n",
       " '+52',\n",
       " '55',\n",
       " '11112222',\n",
       " 'Sofia',\n",
       " ':',\n",
       " 'sofia.hernandez',\n",
       " '@',\n",
       " 'hotmail.com',\n",
       " ',',\n",
       " '+52',\n",
       " '55',\n",
       " '3333',\n",
       " '4444',\n",
       " 'Isabella',\n",
       " ':',\n",
       " 'isabella.garcia',\n",
       " '@',\n",
       " 'outlook.com',\n",
       " ',',\n",
       " '+52',\n",
       " '55',\n",
       " '5555',\n",
       " '6666',\n",
       " 'Miguel',\n",
       " ':',\n",
       " 'miguel.lopez',\n",
       " '@',\n",
       " 'yahoo.com',\n",
       " ',',\n",
       " '+52',\n",
       " '55',\n",
       " '7777',\n",
       " '8888',\n",
       " 'Alejandro',\n",
       " ':',\n",
       " 'alejandro.sanchez',\n",
       " '@',\n",
       " 'gmail.com',\n",
       " ',',\n",
       " '+57',\n",
       " '320',\n",
       " '999',\n",
       " '0000',\n",
       " 'Natalia',\n",
       " ':',\n",
       " 'natalia.gomez',\n",
       " '@',\n",
       " 'hotmail.com',\n",
       " ',',\n",
       " '+57',\n",
       " '311888',\n",
       " '7777']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############## Tokenizacion - el átomo del texto es la palabra################\n",
    "\n",
    "## Tokenizador por palabras\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "texto_tokenizado = word_tokenize(historia)\n",
    "\n",
    "### La funcion word_tokenize de nltk permite tokenizar un texto en palabras, por detrás esta utilizando expresiones regulares para separar las palabras.\n",
    "\n",
    "texto_tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9j26Grc0WXZn",
    "outputId": "107ac84f-d5e3-4683-9010-452a2098a4ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hace',\n",
       " 'varios',\n",
       " 'años',\n",
       " 'en',\n",
       " 'el',\n",
       " 'pelotón',\n",
       " 'de',\n",
       " 'fusilamiento',\n",
       " 'el',\n",
       " 'coronel',\n",
       " 'aureliano',\n",
       " 'buendía',\n",
       " 'había',\n",
       " 'de',\n",
       " 'recordar',\n",
       " 'aquella',\n",
       " 'tarde',\n",
       " 'remota',\n",
       " 'en',\n",
       " 'que',\n",
       " 'su',\n",
       " 'padre',\n",
       " 'lo',\n",
       " 'llevó',\n",
       " 'a',\n",
       " 'conocer',\n",
       " 'el',\n",
       " 'hielo']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenizador por expresiones regulares\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "texto_tokenizado = tokenizer.tokenize(texto)\n",
    "\n",
    "### La clase RegexpTokenizer de nltk permite tokenizar un texto utilizando expresiones regulares, en este caso se tokeniza por palabras.\n",
    "\n",
    "texto_tokenizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3nA-Q6TWXZn"
   },
   "source": [
    "## 5. Eliminación de stopwords\n",
    "\n",
    "\n",
    "Las **stop words** son palabras que tienen poco o ningún valor semántico en un análisis de texto. Estas palabras suelen ser artículos, preposiciones, conjunciones o pronombres que, aunque son esenciales para construir frases en un lenguaje natural, no aportan información relevante para el análisis de contenido en tareas específicas.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Por qué eliminar las stop words?\n",
    "\n",
    "1. **Reducción del ruido en los datos**:\n",
    "   - Elimina palabras comunes que no diferencian un texto de otro, como \"el\", \"de\", \"y\", \"pero\".\n",
    "   - Ejemplo:\n",
    "     ```python\n",
    "     texto = \"El gato está en la casa y duerme.\"\n",
    "     sin_stopwords = \"gato casa duerme\"\n",
    "     ```\n",
    "\n",
    "2. **Optimización de los modelos**:\n",
    "   - Reduce la dimensionalidad del vocabulario, mejorando la eficiencia computacional.\n",
    "   - Ayuda a concentrar el análisis en palabras clave más relevantes.\n",
    "\n",
    "3. **Relevancia en tareas específicas**:\n",
    "   - En tareas como clasificación de texto o clustering, las stop words pueden diluir patrones importantes.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Cuándo evitar eliminarlas?\n",
    "\n",
    "Aunque generalmente es útil, no siempre es adecuado eliminar stop words. Aquí hay casos en los que podrían ser importantes:\n",
    "\n",
    "1. **Análisis de estilo o escritura**:\n",
    "   - En análisis literarios o de autoría, las stop words pueden reflejar patrones estilísticos.\n",
    "   - Ejemplo: En textos de Gabriel García Márquez, el uso de \"y\" es característico de sus frases largas y descriptivas.\n",
    "\n",
    "2. **Modelos avanzados (e.g., BERT, GPT)**:\n",
    "   - Los modelos de lenguaje de gran tamaño ya manejan las stop words en su contexto semántico.\n",
    "   - Eliminarlas podría romper relaciones útiles en tareas como análisis de sentimiento o traducción.\n",
    "\n",
    "3. **Análisis sintáctico**:\n",
    "   - Si el objetivo es analizar estructuras gramaticales, las stop words son esenciales.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Cómo identificar y eliminar stop words?\n",
    "\n",
    "1. **Usando listas predefinidas**:\n",
    "   - Librerías como `nltk` o `spaCy` incluyen listas de stop words comunes.\n",
    "   - Ejemplo con `nltk`:\n",
    "     ```python\n",
    "     from nltk.corpus import stopwords\n",
    "     from nltk.tokenize import word_tokenize\n",
    "     \n",
    "     nltk.download('stopwords')\n",
    "     nltk.download('punkt')\n",
    "\n",
    "     texto = \"El gato está en la casa y duerme.\"\n",
    "     palabras = word_tokenize(texto.lower())\n",
    "     stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "     sin_stopwords = [palabra for palabra in palabras if palabra not in stop_words]\n",
    "     print(sin_stopwords)  # ['gato', 'casa', 'duerme']\n",
    "     ```\n",
    "\n",
    "2. **Personalizando las listas**:\n",
    "   - Puedes agregar o quitar palabras según el dominio del texto.\n",
    "   - Ejemplo: En textos legales, palabras como \"ley\", \"artículo\", \"reglamento\" no deben eliminarse.\n",
    "\n",
    "3. **Usando expresiones regulares**:\n",
    "   - Filtrar palabras que cumplen ciertos patrones.\n",
    "   - Ejemplo: Eliminación de palabras de 1-2 caracteres:\n",
    "     ```python\n",
    "     import re\n",
    "     texto = \"El gato está en la casa y duerme.\"\n",
    "     palabras = re.findall(r'\\b\\w{3,}\\b', texto.lower())\n",
    "     print(palabras)  # ['gato', 'está', 'casa', 'duerme']\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "### Alternativas a eliminar stop words\n",
    "\n",
    "En algunos casos, en lugar de eliminarlas, puedes:\n",
    "1. **Ponderarlas**:\n",
    "   - Usar métodos como TF-IDF que reducen automáticamente el peso de palabras muy frecuentes.\n",
    "\n",
    "2. **Mantenerlas para tareas contextuales**:\n",
    "   - En análisis de sentimiento o detección de sarcasmo, las stop words pueden ser significativas.\n",
    "\n",
    "---\n",
    "\n",
    "La eliminación de stop words es un paso clave en la mayoría de las tareas de NLP, pero debe aplicarse con criterio:\n",
    "\n",
    "- **Útil** en modelos tradicionales como TF-IDF o Word2Vec.\n",
    "- **No siempre necesario** en modelos avanzados como BERT o GPT.\n",
    "- **Personalizable** según el dominio y los objetivos del análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiUZGdjBWXZn"
   },
   "source": [
    "<img src=\"https://www.mermaidchart.com/raw/6c157501-141d-40ae-9295-f991bc567f6e?theme=light&version=v0.1&format=svg\" alt=\"Stop Words\" style=\"width:100%; max-width:1200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TUJUHRHWXZn",
    "outputId": "f407d06d-af37-4e61-9a1c-2cb2b9ecd072"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo',\n",
       " 'como',\n",
       " 'más',\n",
       " 'pero',\n",
       " 'sus',\n",
       " 'le',\n",
       " 'ya',\n",
       " 'o',\n",
       " 'este',\n",
       " 'sí',\n",
       " 'porque',\n",
       " 'esta',\n",
       " 'entre',\n",
       " 'cuando',\n",
       " 'muy',\n",
       " 'sin',\n",
       " 'sobre',\n",
       " 'también',\n",
       " 'me',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'donde',\n",
       " 'quien',\n",
       " 'desde',\n",
       " 'todo',\n",
       " 'nos',\n",
       " 'durante',\n",
       " 'todos',\n",
       " 'uno',\n",
       " 'les',\n",
       " 'ni',\n",
       " 'contra',\n",
       " 'otros',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'ante',\n",
       " 'ellos',\n",
       " 'e',\n",
       " 'esto',\n",
       " 'mí',\n",
       " 'antes',\n",
       " 'algunos',\n",
       " 'qué',\n",
       " 'unos',\n",
       " 'yo',\n",
       " 'otro',\n",
       " 'otras',\n",
       " 'otra',\n",
       " 'él',\n",
       " 'tanto',\n",
       " 'esa',\n",
       " 'estos',\n",
       " 'mucho',\n",
       " 'quienes',\n",
       " 'nada',\n",
       " 'muchos',\n",
       " 'cual',\n",
       " 'poco',\n",
       " 'ella',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'algunas',\n",
       " 'algo',\n",
       " 'nosotros',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'tú',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'ellas',\n",
       " 'nosotras',\n",
       " 'vosotros',\n",
       " 'vosotras',\n",
       " 'os',\n",
       " 'mío',\n",
       " 'mía',\n",
       " 'míos',\n",
       " 'mías',\n",
       " 'tuyo',\n",
       " 'tuya',\n",
       " 'tuyos',\n",
       " 'tuyas',\n",
       " 'suyo',\n",
       " 'suya',\n",
       " 'suyos',\n",
       " 'suyas',\n",
       " 'nuestro',\n",
       " 'nuestra',\n",
       " 'nuestros',\n",
       " 'nuestras',\n",
       " 'vuestro',\n",
       " 'vuestra',\n",
       " 'vuestros',\n",
       " 'vuestras',\n",
       " 'esos',\n",
       " 'esas',\n",
       " 'estoy',\n",
       " 'estás',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'esté',\n",
       " 'estés',\n",
       " 'estemos',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estaré',\n",
       " 'estarás',\n",
       " 'estará',\n",
       " 'estaremos',\n",
       " 'estaréis',\n",
       " 'estarán',\n",
       " 'estaría',\n",
       " 'estarías',\n",
       " 'estaríamos',\n",
       " 'estaríais',\n",
       " 'estarían',\n",
       " 'estaba',\n",
       " 'estabas',\n",
       " 'estábamos',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estuve',\n",
       " 'estuviste',\n",
       " 'estuvo',\n",
       " 'estuvimos',\n",
       " 'estuvisteis',\n",
       " 'estuvieron',\n",
       " 'estuviera',\n",
       " 'estuvieras',\n",
       " 'estuviéramos',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuviese',\n",
       " 'estuvieses',\n",
       " 'estuviésemos',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estando',\n",
       " 'estado',\n",
       " 'estada',\n",
       " 'estados',\n",
       " 'estadas',\n",
       " 'estad',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ha',\n",
       " 'hemos',\n",
       " 'habéis',\n",
       " 'han',\n",
       " 'haya',\n",
       " 'hayas',\n",
       " 'hayamos',\n",
       " 'hayáis',\n",
       " 'hayan',\n",
       " 'habré',\n",
       " 'habrás',\n",
       " 'habrá',\n",
       " 'habremos',\n",
       " 'habréis',\n",
       " 'habrán',\n",
       " 'habría',\n",
       " 'habrías',\n",
       " 'habríamos',\n",
       " 'habríais',\n",
       " 'habrían',\n",
       " 'había',\n",
       " 'habías',\n",
       " 'habíamos',\n",
       " 'habíais',\n",
       " 'habían',\n",
       " 'hube',\n",
       " 'hubiste',\n",
       " 'hubo',\n",
       " 'hubimos',\n",
       " 'hubisteis',\n",
       " 'hubieron',\n",
       " 'hubiera',\n",
       " 'hubieras',\n",
       " 'hubiéramos',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubiese',\n",
       " 'hubieses',\n",
       " 'hubiésemos',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'habiendo',\n",
       " 'habido',\n",
       " 'habida',\n",
       " 'habidos',\n",
       " 'habidas',\n",
       " 'soy',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'somos',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'seas',\n",
       " 'seamos',\n",
       " 'seáis',\n",
       " 'sean',\n",
       " 'seré',\n",
       " 'serás',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'seréis',\n",
       " 'serán',\n",
       " 'sería',\n",
       " 'serías',\n",
       " 'seríamos',\n",
       " 'seríais',\n",
       " 'serían',\n",
       " 'era',\n",
       " 'eras',\n",
       " 'éramos',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'fui',\n",
       " 'fuiste',\n",
       " 'fue',\n",
       " 'fuimos',\n",
       " 'fuisteis',\n",
       " 'fueron',\n",
       " 'fuera',\n",
       " 'fueras',\n",
       " 'fuéramos',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fuese',\n",
       " 'fueses',\n",
       " 'fuésemos',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'sintiendo',\n",
       " 'sentido',\n",
       " 'sentida',\n",
       " 'sentidos',\n",
       " 'sentidas',\n",
       " 'siente',\n",
       " 'sentid',\n",
       " 'tengo',\n",
       " 'tienes',\n",
       " 'tiene',\n",
       " 'tenemos',\n",
       " 'tenéis',\n",
       " 'tienen',\n",
       " 'tenga',\n",
       " 'tengas',\n",
       " 'tengamos',\n",
       " 'tengáis',\n",
       " 'tengan',\n",
       " 'tendré',\n",
       " 'tendrás',\n",
       " 'tendrá',\n",
       " 'tendremos',\n",
       " 'tendréis',\n",
       " 'tendrán',\n",
       " 'tendría',\n",
       " 'tendrías',\n",
       " 'tendríamos',\n",
       " 'tendríais',\n",
       " 'tendrían',\n",
       " 'tenía',\n",
       " 'tenías',\n",
       " 'teníamos',\n",
       " 'teníais',\n",
       " 'tenían',\n",
       " 'tuve',\n",
       " 'tuviste',\n",
       " 'tuvo',\n",
       " 'tuvimos',\n",
       " 'tuvisteis',\n",
       " 'tuvieron',\n",
       " 'tuviera',\n",
       " 'tuvieras',\n",
       " 'tuviéramos',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuviese',\n",
       " 'tuvieses',\n",
       " 'tuviésemos',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'teniendo',\n",
       " 'tenido',\n",
       " 'tenida',\n",
       " 'tenidos',\n",
       " 'tenidas',\n",
       " 'tened',\n",
       " 'que',\n",
       " 'CO']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############## Eliminación de palabras vacias ################\n",
    "\n",
    "## Palabras vacias en español\n",
    "\n",
    "stopwords_esp = stopwords.words('spanish')\n",
    "\n",
    "## Eliminar palabras vacias\n",
    "\n",
    "texto_filtrado = [palabra for palabra in texto_tokenizado if palabra.lower() not in stopwords_esp]\n",
    "\n",
    "### La lista stopwords.words('spanish') contiene las palabras vacias en español, se filtran las palabras vacias del texto tokenizado.\n",
    "\n",
    "stopwords_esp=stopwords_esp+['que','CO']\n",
    "stopwords_esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnJydEa_WXZn",
    "outputId": "df28ac51-05ee-428a-c864-4bc4e28777ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hace',\n",
       " 'varios',\n",
       " 'años',\n",
       " 'pelotón',\n",
       " 'fusilamiento',\n",
       " 'coronel',\n",
       " 'aureliano',\n",
       " 'buendía',\n",
       " 'recordar',\n",
       " 'aquella',\n",
       " 'tarde',\n",
       " 'remota',\n",
       " 'padre',\n",
       " 'llevó',\n",
       " 'conocer',\n",
       " 'hielo']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOmAjbOyWXZn"
   },
   "source": [
    "## 6. Lematización o stemming\n",
    "\n",
    "Tanto la **lemmatización** como el **stemming** son técnicas utilizadas en el preprocesamiento de texto para reducir las palabras a su forma base. Sin embargo, ambas tienen diferencias importantes en su propósito y en cómo se implementan.\n",
    "\n",
    "---\n",
    "\n",
    "### Stemming\n",
    "\n",
    "El **stemming** consiste en reducir una palabra a su raíz o **stem**, eliminando afijos como sufijos y prefijos. Este proceso no garantiza que la raíz sea una palabra válida en el lenguaje, ya que se basa en reglas lingüísticas simples.\n",
    "\n",
    "#### Ejemplo:\n",
    "- Palabras: \"corriendo\", \"corrió\", \"correr\"\n",
    "- Resultado del stemming: \"corr\"\n",
    "\n",
    "#### Ventajas:\n",
    "1. **Rápido**: Se basa en reglas simples y no requiere conocimiento gramatical.\n",
    "2. **Menor consumo de recursos**: Es computacionalmente eficiente.\n",
    "\n",
    "#### Desventajas:\n",
    "1. **Impreciso**: Puede producir raíces que no son palabras reales.\n",
    "   - Ejemplo: \"mejor\" → \"mej\"\n",
    "2. **Desinformativo**: Pierde matices gramaticales, como tiempo verbal o pluralidad.\n",
    "\n",
    "#### Implementación en Python:\n",
    "```python\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "palabras = [\"corriendo\", \"corrió\", \"corre\"]\n",
    "stems = [stemmer.stem(palabra) for palabra in palabras]\n",
    "print(stems)  # ['corr', 'corr', 'corr']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Lemmatización\n",
    "\n",
    "La **lemmatización** reduce las palabras a su forma base o **lema**, considerando su significado y contexto gramatical. Para lograrlo, utiliza un diccionario que identifica el lema correcto de cada palabra.\n",
    "\n",
    "#### Ejemplo:\n",
    "- Palabras: \"corriendo\", \"corrió\", \"correr\"\n",
    "- Resultado de la lematización: \"correr\"\n",
    "\n",
    "#### Ventajas:\n",
    "1. **Precisa**: Retorna palabras válidas en el idioma.\n",
    "2. **Contextualizada**: Considera el significado y la función gramatical.\n",
    "\n",
    "#### Desventajas:\n",
    "1. **Más lenta**: Requiere análisis morfológico y acceso a diccionarios léxicos.\n",
    "2. **Mayor consumo de recursos**: Es computacionalmente más costosa.\n",
    "\n",
    "#### Implementación en Python:\n",
    "Usando `spaCy`:\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "texto = \"Los niños están corriendo rápidamente hacia el parque.\"\n",
    "doc = nlp(texto)\n",
    "lemmatized = [token.lemma_ for token in doc]\n",
    "print(lemmatized)  # ['el', 'niño', 'estar', 'correr', 'rápidamente', 'hacia', 'el', 'parque']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Diferencias clave\n",
    "\n",
    "| Característica      | Stemming                     | Lemmatización                  |\n",
    "|---------------------|------------------------------|--------------------------------|\n",
    "| **Base**            | Reglas lingüísticas simples  | Diccionarios léxicos          |\n",
    "| **Forma resultante**| No siempre es una palabra válida | Siempre retorna una palabra válida |\n",
    "| **Velocidad**       | Más rápido                  | Más lento                     |\n",
    "| **Precisión**       | Menor                       | Mayor                         |\n",
    "| **Uso**             | Análisis rápido y superficial | Tareas que requieren contexto |\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Cuándo usar cada uno?\n",
    "\n",
    "#### **Usa Stemming si...**\n",
    "1. Necesitas procesamiento rápido.\n",
    "2. El contexto no es relevante.\n",
    "3. Estás trabajando con tareas como clustering, donde solo importa la similitud de las raíces.\n",
    "\n",
    "#### **Usa Lemmatización si...**\n",
    "1. La precisión es clave.\n",
    "2. El contexto y la gramática son importantes.\n",
    "3. Estás trabajando con modelos avanzados o tareas como análisis semántico o traducción.\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplo combinado de Stemming y Lemmatización\n",
    "\n",
    "```python\n",
    "from nltk.stem import SnowballStemmer\n",
    "import spacy\n",
    "\n",
    "# Inicializar herramientas\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto = \"Las aves estaban volando por el cielo.\"\n",
    "\n",
    "# Aplicar stemming\n",
    "tokens = texto.split()\n",
    "stems = [stemmer.stem(token) for token in tokens]\n",
    "print(\"Stemming:\", stems)\n",
    "\n",
    "# Aplicar lematización\n",
    "doc = nlp(texto)\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(\"Lemmatización:\", lemmas)\n",
    "```\n",
    "\n",
    "Salida:\n",
    "```\n",
    "Stemming: ['las', 'aves', 'est', 'vol', 'por', 'el', 'ciel']\n",
    "Lemmatización: ['el', 'ave', 'estar', 'volar', 'por', 'el', 'cielo']\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DkFDCvtc1RhC",
    "outputId": "2557d5ff-e639-4d10-c549-444367bb10b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfbat\\anaconda3\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yf5npdyrWXZo",
    "outputId": "05381a4f-0f75-472c-c0db-ea24d3c4a2a6"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m SnowballStemmer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspanish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes_core_news_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Texto de ejemplo\u001b[39;00m\n\u001b[0;32m      5\u001b[0m texto \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLas aves estaban volando por el cielo.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_ean2024\\lib\\site-packages\\spacy\\__init__.py:30\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depr_path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     29\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(Warnings\u001b[38;5;241m.\u001b[39mW001\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdepr_path), \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_ean2024\\lib\\site-packages\\spacy\\util.py:175\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexists\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Path or Path-like to model data\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides)\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto = \"Las aves estaban volando por el cielo.\"\n",
    "\n",
    "# Aplicar stemming\n",
    "tokens = texto.split()\n",
    "stems = [stemmer.stem(token) for token in tokens]\n",
    "print(\"Stemming:\", stems)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kam9eN5GWXZo",
    "outputId": "f61f3584-ce03-4eb1-a77b-69e94b159cc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hac',\n",
       " 'vari',\n",
       " 'años',\n",
       " 'peloton',\n",
       " 'fusil',\n",
       " 'coronel',\n",
       " 'aurelian',\n",
       " 'buend',\n",
       " 'record',\n",
       " 'aquell',\n",
       " 'tard',\n",
       " 'remot',\n",
       " 'padr',\n",
       " 'llev',\n",
       " 'conoc',\n",
       " 'hiel']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############## Stemming ################\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "## Stemmer en español\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "## Stemming\n",
    "\n",
    "texto_stemming = [stemmer.stem(palabra) for palabra in texto_filtrado]\n",
    "\n",
    "### La clase SnowballStemmer de nltk permite realizar stemming en español, se aplica stemming a las palabras filtradas.\n",
    "\n",
    "texto_stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3RvyYFCWXZo"
   },
   "source": [
    "Importante descargar el modelo en español de `spaCy` antes de ejecutar el código:\n",
    "\n",
    "```bash\n",
    "!python -m spacy download es_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfbat\\anaconda3\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNpV2lCtWXZo",
    "outputId": "d6e4b2d4-5bbd-4be1-b18c-bf6641ed04fe"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m############## Lematización ################\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes_core_news_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m## Lematizador en español\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Aplicar lematización\u001b[39;00m\n\u001b[0;32m      7\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(texto_filtrado))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_ean2024\\lib\\site-packages\\spacy\\__init__.py:30\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depr_path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     29\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(Warnings\u001b[38;5;241m.\u001b[39mW001\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdepr_path), \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_ean2024\\lib\\site-packages\\spacy\\util.py:175\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexists\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Path or Path-like to model data\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides)\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "############## Lematización ################\n",
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "## Lematizador en español\n",
    "\n",
    "# Aplicar lematización\n",
    "doc = nlp(' '.join(texto_filtrado))\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(\"Lemmatización:\", lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyV_8P_kWXZo"
   },
   "source": [
    "## 7. Bag of words\n",
    "\n",
    "\n",
    "El modelo **Bag of Words** es una de las técnicas más simples y utilizadas en el procesamiento de lenguaje natural (NLP) para representar texto en un formato numérico que las máquinas pueden entender. A pesar de su simplicidad, sigue siendo una herramienta poderosa para tareas como clasificación de texto, clustering y análisis de sentimiento.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Qué es Bag of Words?\n",
    "\n",
    "El modelo **BoW** convierte un conjunto de textos (corpus) en una representación matricial basada únicamente en la **frecuencia de las palabras** que aparecen en los documentos, ignorando su orden y contexto. En esencia, crea un \"bolso\" de palabras donde la posición de las palabras no importa, pero su presencia o ausencia sí.\n",
    "\n",
    "#### Ejemplo:\n",
    "Corpus:\n",
    "1. \"El gato duerme\"\n",
    "2. \"El perro corre\"\n",
    "3. \"El gato corre\"\n",
    "\n",
    "**Bolso de palabras**:\n",
    "- [\"El\", \"gato\", \"duerme\", \"perro\", \"corre\"]\n",
    "\n",
    "**Matriz de frecuencias**:\n",
    "| Documento    | El  | gato | duerme | perro | corre |\n",
    "|--------------|-----|------|--------|-------|-------|\n",
    "| 1. \"El gato duerme\" | 1   | 1    | 1      | 0     | 0     |\n",
    "| 2. \"El perro corre\" | 1   | 0    | 0      | 1     | 1     |\n",
    "| 3. \"El gato corre\"  | 1   | 1    | 0      | 0     | 1     |\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Cómo funciona?\n",
    "\n",
    "1. **Tokenización**:\n",
    "   - Divide los documentos en palabras (tokens).\n",
    "\n",
    "2. **Construcción del vocabulario**:\n",
    "   - Crea una lista de palabras únicas en el corpus.\n",
    "\n",
    "3. **Vectorización**:\n",
    "   - Representa cada documento como un vector de frecuencias basado en el vocabulario.\n",
    "\n",
    "---\n",
    "\n",
    "### Ventajas\n",
    "\n",
    "1. **Simplicidad**:\n",
    "   - Es fácil de entender e implementar.\n",
    "   \n",
    "2. **Eficiencia**:\n",
    "   - Funciona bien con textos cortos y corpora pequeños.\n",
    "\n",
    "3. **Compatibilidad**:\n",
    "   - Puede integrarse con modelos tradicionales como Naive Bayes, SVM, o redes neuronales básicas.\n",
    "\n",
    "---\n",
    "\n",
    "### Desventajas\n",
    "\n",
    "1. **Ignora el contexto**:\n",
    "   - No captura relaciones entre palabras, como sinónimos o frases.\n",
    "   \n",
    "2. **Alta dimensionalidad**:\n",
    "   - Si el corpus tiene muchas palabras únicas, el vector resultante será grande y disperso.\n",
    "\n",
    "3. **No distingue la importancia de las palabras**:\n",
    "   - Palabras comunes como \"el\", \"de\", \"y\" tienen el mismo peso que palabras más significativas.\n",
    "\n",
    "---\n",
    "\n",
    "### Mejoras al modelo BoW\n",
    "\n",
    "1. **TF-IDF (Term Frequency-Inverse Document Frequency)**:\n",
    "   - Ajusta las frecuencias para dar más peso a palabras relevantes y reducir el peso de palabras comunes.\n",
    "   \n",
    "2. **N-grams**:\n",
    "   - Considera secuencias de palabras (como pares o tríos) en lugar de palabras individuales.\n",
    "\n",
    "3. **Reducción de dimensionalidad**:\n",
    "   - Usa técnicas como PCA o selección de características para manejar la alta dimensionalidad.\n",
    "\n",
    "---\n",
    "\n",
    "### Implementación en Python\n",
    "\n",
    "#### Usando `CountVectorizer` de `sklearn`\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Corpus de ejemplo\n",
    "corpus = [\n",
    "    \"El gato duerme\",\n",
    "    \"El perro corre\",\n",
    "    \"El gato corre\"\n",
    "]\n",
    "\n",
    "# Inicializar el vectorizador\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transformar el corpus en una matriz BoW\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Mostrar el vocabulario\n",
    "print(\"Vocabulario:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Mostrar la matriz BoW\n",
    "print(\"Matriz BoW:\\n\", X.toarray())\n",
    "```\n",
    "\n",
    "**Salida**:\n",
    "```\n",
    "Vocabulario: ['corre' 'duerme' 'el' 'gato' 'perro']\n",
    "Matriz BoW:\n",
    " [[0 1 1 1 0]\n",
    "  [1 0 1 0 1]\n",
    "  [1 0 1 1 0]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Cuándo usar Bag of Words?\n",
    "\n",
    "1. **Tareas sencillas**:\n",
    "   - Clasificación de texto, análisis de sentimiento o detección de spam.\n",
    "\n",
    "2. **Modelos tradicionales**:\n",
    "   - Funciona bien con Naive Bayes, SVM o regresión logística.\n",
    "\n",
    "3. **Corpus pequeños**:\n",
    "   - En textos largos o complejos, su incapacidad para capturar contexto se vuelve un problema.\n",
    "\n",
    "---\n",
    "\n",
    "- **Simplicidad**: Fácil de entender y usar.\n",
    "- **Limitaciones**: Ignora contexto y puede generar vectores grandes.\n",
    "- **Alternativas**: TF-IDF o embeddings como Word2Vec y BERT para modelos más avanzados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152
    },
    "id": "B40c9ey1WXZp",
    "outputId": "03c6b13f-d9b3-4d4a-91be-253996a2dfa2"
   },
   "outputs": [],
   "source": [
    "############## Bag of words ################\n",
    "\n",
    "## Vectorizador\n",
    "\n",
    "vectorizador = CountVectorizer()\n",
    "\n",
    "## Bolsa de palabras\n",
    "\n",
    "texto_bow = vectorizador.fit_transform([' '.join(lemmas)])\n",
    "\n",
    "## Veamos un dataframe con la bolsa de palabras\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(texto_bow.toarray(), columns=vectorizador.get_feature_names_out())\n",
    "df_index = pd.DataFrame([' '.join(lemmas)], columns=['Texto'])\n",
    "\n",
    "df = pd.concat([df_index, df], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7NTsQvLWXZp"
   },
   "source": [
    "## 8. Conclusiones\n",
    "\n",
    "En este notebook, se presentaron los pasos más comunes en el preprocesamiento de texto. Es importante tener en cuenta que estos pasos pueden variar dependiendo del problema y del texto que se esté analizando. Para las siguientes tareas es probable que se hagan algunos de los pasos mencionados anteriormente.\n",
    "\n",
    "<img src=\"https://www.mermaidchart.com/raw/b60c8142-12ae-46c9-8412-7333ee1508cd?theme=light&version=v0.1&format=svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYVnVKN35SUw"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  # Normalización\n",
    "  text = text.lower()\n",
    "\n",
    "  # regex clean\n",
    "  text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "  # Tokenización\n",
    "  tokens = word_tokenize(text)\n",
    "\n",
    "  # Stop words\n",
    "  stopwords_esp = stopwords.words('spanish')\n",
    "  tokens = [token for token in tokens if token not in stopwords_esp]\n",
    "\n",
    "  ## Lemmatize\n",
    "\n",
    "  nlp=spacy.load(\"es_core_news_sm\")\n",
    "  doc = nlp(' '.join(tokens))\n",
    "  tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "  return ' '.join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FyskzQSu9cJU",
    "outputId": "347f7a2f-1681-466e-bb49-60382c0e4c2a"
   },
   "outputs": [],
   "source": [
    "clean_text('En la vida hay muchos chismoso para las chismofilias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "u4WClQx75WyW",
    "outputId": "29b87564-4b9a-46d8-c495-56128980fa74"
   },
   "outputs": [],
   "source": [
    "Corpus_DF['Texto_Limpio'] = Corpus_DF['Comentario'].apply(clean_text)\n",
    "Corpus_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "FFcdTVuK6IS3",
    "outputId": "8a57f861-af0f-492e-c59d-cee27e41d657"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "vectorizador_chismofilia= CountVectorizer()\n",
    "\n",
    "vectorizador_chismofilia.fit(Corpus_DF['Texto_Limpio'])\n",
    "\n",
    "salida=vectorizador_chismofilia.transform(Corpus_DF['Texto_Limpio'])\n",
    "\n",
    "pd.DataFrame(salida.toarray(), columns=vectorizador_chismofilia.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "no0mHgfX9-PK",
    "outputId": "2773cc3b-17c2-47e5-937a-507bc659af15"
   },
   "outputs": [],
   "source": [
    "Corpus_DF['Texto_Limpio'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "nvxhgT7q6pwr",
    "outputId": "2fa0e32c-4008-4b96-ea95-0c612a8b3c25"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "\n",
    "  nlp=spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "    # Stop words\n",
    "  stopwords_esp = stopwords.words('spanish')\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, X, y=None):\n",
    "    return self\n",
    "\n",
    "  def clean_text(self,text):\n",
    "    # Normalización\n",
    "    text = text.lower()\n",
    "    # regex clean\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stopwords_esp]\n",
    "\n",
    "    ## Lemmatize\n",
    "    \n",
    "    doc = nlp(' '.join(tokens))\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "  def transform(self, X, y=None):\n",
    "    return X.apply(self.clean_text)\n",
    "\n",
    "  def fit_transform(self, X, y=None):\n",
    "    return self.transform(X)\n",
    "\n",
    "cleaner=CleanText()\n",
    "\n",
    "cleaner.fit_transform(Corpus_DF['Comentario'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYV0UHKI8hAm"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('cleaner', CleanText()),\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "mt0BIy7M_pZy",
    "outputId": "b0975374-526d-4dbf-eead-df4201ac7a6b"
   },
   "outputs": [],
   "source": [
    "pipeline.fit(Corpus_DF['Comentario'], Corpus_DF['Sentimiento']=='Positivo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GlRAH4b_09h"
   },
   "outputs": [],
   "source": [
    "y_pred=pipeline.predict(Corpus_DF['Comentario'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xq6bjkquAf9Y",
    "outputId": "d223f667-4803-44a9-e33f-2a065738e0b6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(Corpus_DF['Sentimiento']=='Positivo', y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DL8LVcEZAosH",
    "outputId": "8f4affbe-ff32-4e0c-d285-e0c503202054"
   },
   "outputs": [],
   "source": [
    "pipeline.predict(pd.DataFrame(['Es un producto frustrante'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQWhuw6YCWp3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "9U2tLrM_CVMw",
    "outputId": "93e09bb5-28e1-44ba-f2e6-a942bede9438"
   },
   "outputs": [],
   "source": [
    "cleaner=CleanText()\n",
    "cleaner.fit(Corpus_DF['Comentario'])\n",
    "\n",
    "count_vectorizer=CountVectorizer()\n",
    "count_vectorizer.fit(cleaner.transform(Corpus_DF['Comentario']))\n",
    "\n",
    "random_forest_classifier=RandomForestClassifier()\n",
    "random_forest_classifier.fit(count_vectorizer.transform(cleaner.transform(Corpus_DF['Comentario'])), Corpus_DF['Sentimiento']=='Positivo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "id": "Ryim8QgsCc8A",
    "outputId": "ef09ceff-bda1-4386-b76f-c62146117539"
   },
   "outputs": [],
   "source": [
    "texto=cleaner.transform(pd.DataFrame(['Es un producto frustrante'])[0])\n",
    "\n",
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "id": "Ye7r74uHC6hg",
    "outputId": "4c2f5662-fc27-411d-c5f5-a56e0cea5ee6"
   },
   "outputs": [],
   "source": [
    "vector=count_vectorizer.transform(texto)\n",
    "\n",
    "pd.DataFrame(vector.toarray(), columns=count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "faHz64skAxYQ",
    "outputId": "97b05453-dcad-4eb4-a860-4d20b69b8b7c"
   },
   "outputs": [],
   "source": [
    "random_forest_classifier.predict(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7DfZQJ4A5JM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DF=pd.read_excel('../../Datos/Datos Crudos/reviews hotel total.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF[['País', 'Acomodación', 'Noches', 'Fecha hospedaje',\n",
    "       'Grupo viaje', 'Fecha reseña', 'Titulo', 'Calificación',\n",
    "       'Cosas Positivas', 'Cosas Negativas', 'reseña']].to_csv('reviews_booking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['Calificación']=DF['Calificación'].str.replace(',','.').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_inicial=DF[DF['Calificación']<=7]\n",
    "\n",
    "DF_inicial=DF_inicial[DF_inicial['País'].isin(['Colombia'])]['Cosas Negativas']\n",
    "DF_inicial=DF_inicial[DF_inicial.fillna('').apply(len)>60].sample(7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Final=pd.DataFrame()\n",
    "\n",
    "DF_Final['Comentarios']=DF_inicial\n",
    "DF_Final['Clas']='Negativos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_limpio=cleaner.fit_transform(DF_inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count_vectorizer=TfidfVectorizer(ngram_range=(2,2))\n",
    "vectores=count_vectorizer.fit_transform(text_limpio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DF_count=pd.DataFrame(vectores.toarray(),columns=count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_count.sum(axis=0).sort_values(ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_inicial=DF[DF['Calificación']>=9]\n",
    "\n",
    "DF_inicial=DF_inicial[DF_inicial['País'].isin(['Colombia'])]['Cosas Positivas']\n",
    "DF_inicial=DF_inicial[DF_inicial.fillna('').apply(len)>60].sample(7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Final_2=pd.DataFrame()\n",
    "\n",
    "DF_Final_2['Comentarios']=DF_inicial\n",
    "DF_Final_2['Clas']='Positivos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Final_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_limpio=cleaner.fit_transform(DF_Final_2['Comentarios'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count_vectorizer=TfidfVectorizer(ngram_range=(2,2))\n",
    "vectores=count_vectorizer.fit_transform(text_limpio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DF_count=pd.DataFrame(vectores.toarray(),columns=count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_count.sum(axis=0).sort_values(ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "history_visible": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlpean",
   "language": "python",
   "name": "nlpean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
